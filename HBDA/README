This is the hiererachical BiLSTM Word Attention Model. The main innovation is to use the Siamese_LSTM + word attention + Manhattan distance compare to the normal Siamese_LSTM + normal attention + Manhattan distance.

The AttentionLayer is referred from the following link.
https://github.com/uhauha2929/examples/blob/master/Hierarchical%20Attention%20Networks%20.ipynb

The Siamese_LSTM network and Manhattan distance are referred from the following links.
https://zhuanlan.zhihu.com/p/31638132
https://github.com/likejazz/Siamese-LSTM
https://github.com/LuJunru/Sentences_Pair_Similarity_Calculation_Siamese_LSTM
